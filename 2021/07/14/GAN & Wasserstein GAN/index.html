<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>GAN &amp; Wasserstein GAN</title><meta name="description" content="Only The Paranoids Survive."><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.png"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="
A generator G and a discriminator D

G’s goal: minimise objective such that $D(G(z))$ is close to 1 (discriminator is fooled into thinking generated $G(z)$ is real)
D’s goal: maximise objective such that $D(x)$ is close to 1 (real) and $D(G(z))$ is close to 0 (fake)
G and D are both neural networks, G is differentiable
Alternate between “Gradient Ascent o.."><meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Phillip's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">GAN &amp; Wasserstein GAN</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile is-hidden"></div><div class="column is-9"><header class="my-4"><a href="/tags/GAN"><i class="tag post-item-tag">GAN</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">GAN &amp; Wasserstein GAN</h1><time class="has-text-grey" datetime="2021-07-14T09:31:45.000Z">2021-07-14</time><article class="mt-2 post-content"><ul>
<li><p>A generator G and a discriminator D</p>
<ul>
<li>G’s goal: minimise objective such that $D(G(z))$ is close to 1 (discriminator is fooled into thinking generated $G(z)$ is real)</li>
<li>D’s goal: maximise objective such that $D(x)$ is close to 1 (real) and $D(G(z))$ is close to 0 (fake)</li>
<li>G and D are both neural networks, G is differentiable</li>
<li>Alternate between “Gradient Ascent on D” and “Gradient Descent on G”</li>
<li>Optimisation target:<br>$$\min <em>{G} \max <em>{D} V(D, G)=\mathbb{E}</em>{\boldsymbol{x} \sim p</em>{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}<em>{\boldsymbol{z} \sim p</em>{\boldsymbol{z}}(\boldsymbol{z})}[\log (1-D(G(\boldsymbol{z})))]$$</li>
<li>The following figure shows the process of optimisation. Actually, we are looking for a map between $z$ and $x$. $z$ could be any distribution, but normally we use normal distribution. The purpose of GAN is to find out a way to transform a distribution from which $z$ is sampled to the target ($p_{data}$). Ideally, at the last step, the distribution of the generated data ($p_g$) is 100% the same as $p_{data}$, D is not able to descriminate the generated data from the real data and hence output 0.5.<br><img src="https://i.imgur.com/zN0wr1r.png" alt="zN0wr1r"></li>
</ul>
</li>
<li><p>Backgroud Knowledge</p>
<ul>
<li>KL divergence - to measure the similarity of two distributions. Smaller number means they are closer.<ul>
<li>discrete<br>$$D_{K L}(P | Q)=\sum_{i} P(i) \log \frac{P(i)}{Q(i)}=\mathbb{E}[\log\frac{P(i)}{Q(i)}]$$</li>
<li>continuous<br>$$D_{K L}(P | Q)=\int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} d x=\mathbb{E}[\log\frac{p(x)}{q(x)}]$$</li>
<li>Note that they can be expressed as expectations.</li>
</ul>
</li>
<li>JS divergence - another distribution similarity measurement derived from KL divergence.<br>$$D_{JS}(P||Q)=\frac{1}{2} \mathrm{KL}(p | m)+\frac{1}{2} \mathrm{KL}(q | m),\ where\ m(x)=\frac{1}{2}(p(x)+q(x))$$</li>
</ul>
</li>
<li><p>Difficulty of GANs</p>
<ul>
<li>The gradient vanishing<ul>
<li>At the very early phase of training, D is very easy to be confident in detecting G, so D will output almost always 0</li>
<li>In GAN, better discriminator leads to worse vanishing gradient in its generator</li>
</ul>
</li>
</ul>
</li>
<li><p>JS divergence</p>
<ul>
<li>$JS(P_r||P_G)=log2$ when two distribution has no overlap</li>
<li>The probability that the support of $p_{data}$ and $p_g$ have almost zero overlap is 1.<ul>
<li>support(支撑集) - 函数的非零部分子集，比如ReLu函数的支撑集就是 $(0, +\infty)$，一个概率分布的支撑集就是所有概率密度非零部分的集合</li>
</ul>
</li>
</ul>
</li>
<li><p>Wasserstein (Earth-Mover) distance</p>
<p>  $$W(P_{data}, P_{g})=\inf_{\gamma\sim\Pi(P_{data}, P_{g})} \mathbb{E}_{(x, y)\sim\gamma}[||x-y||]$$</p>
<ul>
<li>$\Pi(P_{data},P_g)$ 是$P_{data}$和$P_g$组合起来的所有可能的联合分布的集合，即$\Pi(P_{data},P_g)$中每一个分布的边缘分布都是$P_{data}$和$P_g$</li>
<li>直观理解$P_{data}$和$P_g$像两堆土，位置不一样，形状不一样， $W(P_{data}, P_{g})$代表把土从$P_{data}$挪到$P_g$所需要的最小消耗</li>
<li>和KL散度，JS散度相比，优越性在于，即便两个分布没有重叠，Wasserstein distance仍然能够反映它们的远近</li>
</ul>
</li>
<li><p>Loss Function in WGAN</p>
<ul>
<li><p>the infimum is highly intractable</p>
</li>
<li><p>convert the W-distance to its duality form</p>
<p>  $$\begin{aligned}W(P_{data}, P_{g})&amp;=\sup_{||f||<em>{L}\le 1}\mathbb{E}</em>{x\sim P_{data}}[f(x)]-\mathbb{E}<em>{x\sim P</em>{g}}[f(x)] \&amp;=\frac{1}{K}\sup_{||f||<em>{L}\le K}\mathbb{E}</em>{x\sim P_{r}}[f(x)]-\mathbb{E}<em>{x\sim P</em>{g}}[f(x)]\end{aligned}$$</p>
</li>
<li><p>Lipschitz连续 - 在一个连续函数 $f$ 上面额外施加了一个限制，要求存在一个常数 $K\ge0$ 使得定义域内的任意两个元素 $x_1$ 和 $x_2$ 都满足 $|f(x_{1})-f(x_{2})| \leq K|x_{1}-x_{2}|$, 此时称函数 $f$ 的Lipschitz常数为 $K$. 简单理解，比如说 $f$ 的定义域是实数集合，那上面的要求就等价于 $f$ 的导函数绝对值不超过 $K$。Lipschitz连续条件限制了一个连续函数的最大局部变动幅度。</p>
</li>
<li><p>对偶式的意思就是在要求函数 $f$ 的Lipschitz常数 $||f||<em>L$ 不超过 $K$ 的条件下，对所有可能满足条件的 $f$ 取到 $\mathbb{E}</em>{x \sim P_{data}}[f(x)]-\mathbb{E}<em>{x \sim P</em>{g}}[f(x)]$ 的上界，然后再除以 $K$。</p>
</li>
<li><p>把函数 $f$ 用一个带参数的神经网络来表示</p>
<p>  $$W(P_{data}, P_{g})=\max <em>{w \in W} \mathbb{E}</em>{x \sim P_{data}}[f_{w}(x)]-\mathbb{E}<em>{x \sim P</em>{g}}[f_{w}(x)]$$</p>
</li>
<li><p>$w$ 还要满足原来的限制 $||f_w||_L\le K$，但我们其实不关心具体的K是多少，它只是会使得梯度变大K倍，并不会影响梯度的方向。此处简化为限制所有的参数 $w$ 不超过某个范围 $[-c,c]$，此时关于输入样本 $x$ 的导数 $\frac{\partial f_w}{\partial x}$ 也不会超过某个范围，所以一定存在某个不知道的常数 $K$ 使得 $f_w$ 的局部变动幅度不会超过它，Lipschitz连续条件得以满足。</p>
</li>
<li><p>The loss of discriminator (Remove the last sigmoid layer)</p>
<p>  $$L_d=\mathbb{E}<em>{x \sim P</em>{data}}[f_{w}(x)]-\mathbb{E}<em>{x \sim P</em>{g}}[f_{w}(x)]$$</p>
</li>
<li><p>$f_w$’s goal: maximise Wasserstein distance between real data distribution and generative distribution</p>
</li>
<li><p>The Loss of generator</p>
<p>  $$L_g=-\mathbb{E}<em>{x \sim P</em>{g}}[f_{w}(x)]=-\mathbb{E}<em>{z \sim p(z)}[f</em>{w}(g_{\theta}(z))]$$</p>
</li>
<li><p>$g_\theta$’s goal: minimise Wasserstein distance between real data distribution and generative distribution</p>
</li>
</ul>
</li>
<li><p>Compare to GAN</p>
<ul>
<li>Remove the last sigmoid</li>
<li>no log is applied to the loss</li>
<li>limit the parameters $w$ within a certain range $[-c,c]$ (weight clipping)</li>
<li></li>
</ul>
</li>
</ul>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2021/07/15/HMM/" title="HMM"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: HMM</span></a></section><article class="mt-6 comment-container"><script async repo="jsxhhyf/jsxhhyf.github.io" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/https://github.com/jsxhhyf"><i class="iconfont icon-github"></i></a><!-- Ins--><!-- RSS--><!-- 知乎--><!-- 领英--><a title="linkedin" target="_blank" rel="noopener nofollow" href="//www.linkedin.com/in/https://www.linkedin.com/in/yifei-hu-1419b1b4/?locale=en_US"><i class="iconfont icon-linkedin"></i></a><!-- 脸书--></section><p><span>Copyright ©</span><span> Phillip 2021</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/post.js"></script></body></html>